# AlertManager configuration for LLM Latency Lens

global:
  resolve_timeout: 5m
  smtp_from: 'alertmanager@example.com'
  smtp_smarthost: 'localhost:25'
  smtp_require_tls: false

# Templates for alert notifications
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route tree for alert routing
route:
  # Default receiver
  receiver: 'default'

  # Group alerts by these labels
  group_by: ['alertname', 'cluster', 'service']

  # Wait time before sending initial notification
  group_wait: 10s

  # Wait time before sending notification about new alerts in group
  group_interval: 10s

  # Wait time before re-sending notification
  repeat_interval: 12h

  # Child routes
  routes:
    # Critical alerts - immediate notification
    - match:
        severity: critical
      receiver: 'critical'
      group_wait: 10s
      group_interval: 5m
      repeat_interval: 4h

    # Warning alerts - batched notification
    - match:
        severity: warning
      receiver: 'warning'
      group_wait: 30s
      group_interval: 10m
      repeat_interval: 12h

    # LLM-specific alerts
    - match:
        component: llm-latency-lens
      receiver: 'llm-team'
      group_by: ['alertname', 'provider']

# Inhibition rules (suppress alerts)
inhibit_rules:
  # Suppress warning if critical is firing
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'cluster', 'service']

  # Suppress latency alerts if service is down
  - source_match:
      alertname: 'ServiceDown'
    target_match_re:
      alertname: '.*Latency.*'
    equal: ['instance']

# Receivers (notification destinations)
receivers:
  # Default receiver - logs only
  - name: 'default'
    webhook_configs:
      - url: 'http://localhost:5001/webhook'
        send_resolved: true

  # Critical alerts
  - name: 'critical'
    # Slack notifications
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#alerts-critical'
        title: 'Critical Alert: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'

    # Email notifications
    email_configs:
      - to: 'oncall@example.com'
        from: 'alertmanager@example.com'
        subject: '[CRITICAL] {{ .GroupLabels.alertname }}'
        html: '{{ range .Alerts }}{{ .Annotations.description }}<br/>{{ end }}'
        send_resolved: true

    # PagerDuty integration
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: '{{ .GroupLabels.alertname }}'
        severity: 'critical'

  # Warning alerts
  - name: 'warning'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#alerts-warning'
        title: 'Warning: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true
        color: 'warning'

  # LLM team notifications
  - name: 'llm-team'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#llm-monitoring'
        title: 'LLM Alert: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true

    email_configs:
      - to: 'llm-team@example.com'
        from: 'alertmanager@example.com'
        subject: '[LLM] {{ .GroupLabels.alertname }}'
        html: '{{ range .Alerts }}{{ .Annotations.description }}<br/>{{ end }}'
